create_plan:
  description: |
    You are given a causal question or a dataset with a user query. Begin by identifying the treatment(s), outcome(s),
    and any relevant confounders or domain assumptions. Use this information to formulate a causal analysis plan consisting
    of structured high-level subgoals, each targeting a phase in the pipeline such as:
    - Background literature search and constraint identification
    - Data preprocessing and diagnostics
    - Causal graph discovery (e.g., PC, NOTEARS, LiNGAM)
    - Validation using cross-evidence or simulation
    - Causal inference (e.g., effect estimation)
    - Result interpretation and reporting

    For each subgoal, specify the objective and expected output only. Do not assign agents — this will be handled in later steps.
    Write the resulting subgoal list to 'SubGoals.json'.

    User query: {causal_query}
    Dataset: {dataset_name}
  expected_output: |
    A file named 'SubGoals.json' containing a list of 4-6 high-level subgoals tailored to the causal question. Each subgoal must include:

    - description: a clear, concise goal (e.g., \"Run causal discovery on the cleaned dataset\")
    - expected_output: the expected deliverable for the subgoal (e.g., \"a DAG with estimated edge weights and adjacency matrix\")
    - status: always initialized as 'pending'
    - score: null
    - feedback: null
    - combined_results: null

    Example structure:
    [
      {
        "id": 1,
        "description": "Perform literature review on potential confounders between variable X and Y",
        "expected_output": "A list of at least 3 relevant papers and a summary of domain constraints",
        "status": "pending",
        "score": null,
        "feedback": null,
        "combined_results": null,
      },
      ...
    ]
  agent: scientist_agent

start_subgoal:
  description: |
    Read 'SubGoals.json' and identify the first subgoal where status != 'completed' and (score == null or score < {threshold}). Utilize feedback from the Scientist Agent if available.
    Decompose this subgoal into 2-4 smaller, very concrete subsubgoals that can be executed independently, each meant for and assigned to a specialized agent (for now, assume all are for dummy_specialized_agent).

    For example:
    - If the subgoal involves causal discovery, assign:
      • Data diagnostics to CodingAgent
      • Run NOTEARS algorithm to DiscoveryAgent
      • Literature cross-check to VerificationAgent
    - If the subgoal involves validation, assign:
      • Literature search to SearchAgent
      • Counterfactual simulation to VerificationAgent

    For now, assume all are for dummy_specialized_agent

    Save the resulting subsubgoals to a file named 'SubSubGoals.json'. Each subsubgoal must include:
    - id: unique subsubgoal ID
    - parent_id: corresponding subgoal ID
    - description: a detailed, concrete task definition
    - assigned_agent: name of the agent responsible (for now, assume all are for dummy_specialized_agent)
    - status: initialized as 'pending'
    - tool_hints: optional suggestions (e.g., algorithm or dataset to use)
    - output: null

    Also, update the corresponding subgoal in 'SubGoals.json' to status='in_progress'.

    Threshold: {threshold}
  expected_output: |
    'SubSubGoals.json' containing 2-4 well-defined subsubgoals, each with:
    - id
    - parent_id
    - description
    - assigned_agent
    - status='pending'
    - Optional: tool_hints
    - output: null

    'SubGoals.json' is updated with the selected subgoal marked as status='in_progress'.
  agent: assistant_agent

collect_subgoal_results:
  description: |
    Read 'SubSubGoals.json' and identify all subsubgoals with status='completed'. Group them by their parent_id.

    For each group:
    1. Synthesize a comprehensive summary of the outputs based on subgoal type:
        - *Discovery*: include DAG or CPDAG structure, algorithm used, edge confidence, and any assumptions or limitations
        - *Validation*: include literature support, consistency checks, and simulation results
        - *Inference*: include effect estimates, standard errors, confidence intervals, and assumptions

    2. Write the synthesized summary into the parent subgoal (in 'SubGoals.json') under the field combined_result.

    3. Mark the parent subgoal's status as 'for_review'.

    Format the summary according to the requested style:
    - 'structured': use numbered bullet points and section headers
    - 'narrative': use paragraph-style explanation with contextual transitions

    Summary style: {summary_style}
  expected_output: |
    'SubGoals.json' is updated with:
    - combined_result: a synthesized, human-readable summary for each updated subgoal
    - status='for_review' for each parent subgoal
  agent: assistant_agent

evaluate_subgoal:
  description: |
    Read 'SubGoals.json' and identify all subgoals with status='for_review'. For each, review the combined_result field and evaluate its quality using the following criteria:

    Evaluation by subgoal type:
    - *Discovery*: assess graph clarity, validity of methods, and confidence in inferred relationships
    - *Validation*: examine consistency across literature, simulations, and assumptions
    - *Inference*: verify interpretability of causal effect estimates and robustness of assumptions

    Use the following weighted scoring rubric to assign a score between 0.0 and 1.0 for each subgoal:

    Scoring Rubric (rate each on 0.0-1.0 scale):

    1. Clarity and completeness of output (weight: 0.25)  
       - Are key components (results, assumptions, methodology) clearly presented?

    2. Methodological soundness (weight: 0.30)  
       - Are methods appropriate and properly applied?

    3. Evidence strength or support (weight: 0.25)  
       - Is the result backed by sufficient data, literature, or simulation?

    4. Interpretability and usefulness (weight: 0.20)  
       - Is the result understandable and actionable for the main causal query?

    Compute the final score as a weighted average. Interpret scores as follows:
    - 0.85 - 1.0: Excellent. No revision needed.
    - 0.70 - 0.84: Good. Acceptable but could be improved.
    - 0.50 - 0.69: Marginal. Needs refinement.
    - < 0.50: Inadequate. Subgoal must be redone.

    For each subgoal:
    1. Assign a score between 0.0 and 1.0.
    2. Provide concise but specific feedback highlighting strengths or deficiencies.
    3. Compare the score to the threshold:
        - If score >= {threshold}, mark the subgoal as 'completed'
        - If score < {threshold}, mark it 'pending' and suggest refinements

    Re-evaluate the entire plan by looking at the subgoals. If deemed necessary, make changes to the plan. Remove or add subgoals if necessary. Maintain the json fields of the unchanged subgoals as they are, such as status, combined results, etc.

    Save all updates back to 'SubGoals.json'.

    Important note: for now, provide a score of 1.0 no matter what output is returned by the dummy_specialized_agent.

    Evaluation threshold: {threshold}
  expected_output: |
    'SubGoals.json' is updated with:
    - score: float between 0.0 and 1.0
    - feedback: evaluator's written response
    - status: set to 'completed' or 'pending' based on whether the score meets the threshold
    for each subgoal with status='for_review'.
  agent: scientist_agent

run_dummy_agent_task:
  description: |
    Read 'SubSubGoals.json' and find all subsubgoals assigned to 'dummy_specialized_agent' with status='pending'. For each:
    - Mark the subsubgoal as 'completed'
    - Add a dummy output such as "This is a dummy result."
    - Save the result in the field called 'output'

    This agent always pretends the task was executed perfectly and returns maximal quality content, no matter the actual task.

    Save the updated subsubgoals back to 'SubSubGoals.json'.
  expected_output: |
    Each 'dummy_specialized_agent' subsubgoal in 'SubSubGoals.json' is updated with:
    - status='completed'
    - output: dummy string indicating the task was successfully faked
  agent: dummy_specialized_agent
